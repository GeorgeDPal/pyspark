{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8726a3cd",
   "metadata": {},
   "source": [
    "# importing pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99804b61",
   "metadata": {},
   "source": [
    "pyspark is a Python API for Spark that lets you harness the simplicity of Python and the power of Apache Spark to scale up the analytical computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6c0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac4065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "# check pyspark version\n",
    "print(ps.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573ee89",
   "metadata": {},
   "source": [
    "# creating spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2da877",
   "metadata": {},
   "source": [
    "spark session is the entry point to programming Spark with the Dataset and DataFrame API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25709c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/28 08:27:04 WARN Utils: Your hostname, codespaces-72158f, resolves to a loopback address: 127.0.0.1; using 10.0.0.169 instead (on interface eth0)\n",
      "25/07/28 08:27:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/28 08:27:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pyspark_practice\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d084d7",
   "metadata": {},
   "source": [
    "# uploading the dataset and creating dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b90e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data here is a list of tuples\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 30, \"Engineering\", 85000, \"2018-03-15\", \"Bangalore\", 4.5),\n",
    "    (2, \"Bob\", 45, \"Sales\", 70000, \"2016-07-01\", \"Mumbai\", 3.8),\n",
    "    (3, \"Charlie\", 28, \"Marketing\", 65000, \"2019-11-23\", \"Delhi\", 4.2),\n",
    "    (4, \"David\", 35, \"Engineering\", 92000, \"2015-04-11\", \"Bangalore\", 4.7),\n",
    "    (5, \"Eva\", 40, \"HR\", 60000, \"2017-02-01\", \"Pune\", 3.9),\n",
    "    (6, \"Frank\", 29, \"Sales\", 68000, \"2020-09-20\", \"Chennai\", 4.1),\n",
    "    (7, \"Grace\", 33, \"Engineering\", 87000, \"2018-12-12\", \"Hyderabad\", 4.6),\n",
    "    (8, \"Hannah\", 31, \"Marketing\", 64000, \"2019-05-25\", \"Delhi\", 4.0),\n",
    "    (9, \"Ian\", 38, \"Sales\", 72000, \"2014-10-10\", \"Mumbai\", 3.7),\n",
    "    (10, \"Jenny\", 27, \"HR\", 59000, \"2021-01-01\", \"Bangalore\", 4.3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the schema for the DataFrame using StructType and StructField\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"joining_date\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"performance_rating\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a48915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78335ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+------------+---------+------------------+\n",
      "| id|   name|age| department|salary|joining_date| location|performance_rating|\n",
      "+---+-------+---+-----------+------+------------+---------+------------------+\n",
      "|  1|  Alice| 30|Engineering| 85000|  2018-03-15|Bangalore|               4.5|\n",
      "|  2|    Bob| 45|      Sales| 70000|  2016-07-01|   Mumbai|               3.8|\n",
      "|  3|Charlie| 28|  Marketing| 65000|  2019-11-23|    Delhi|               4.2|\n",
      "|  4|  David| 35|Engineering| 92000|  2015-04-11|Bangalore|               4.7|\n",
      "|  5|    Eva| 40|         HR| 60000|  2017-02-01|     Pune|               3.9|\n",
      "|  6|  Frank| 29|      Sales| 68000|  2020-09-20|  Chennai|               4.1|\n",
      "|  7|  Grace| 33|Engineering| 87000|  2018-12-12|Hyderabad|               4.6|\n",
      "|  8| Hannah| 31|  Marketing| 64000|  2019-05-25|    Delhi|               4.0|\n",
      "|  9|    Ian| 38|      Sales| 72000|  2014-10-10|   Mumbai|               3.7|\n",
      "| 10|  Jenny| 27|         HR| 59000|  2021-01-01|Bangalore|               4.3|\n",
      "+---+-------+---+-----------+------+------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() # display the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b2c263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor example:\\nStructType([StructField(\"column_name\", DataType(), nullable),])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StructType is a data type that defines a schema for a DataFrame.\n",
    "# It describes the names, data types, and nullability of each column in a structured format\n",
    "# StructField is a component of StructType that defines a single field in the schema.\n",
    "\n",
    "# In PySpark, nullability refers to whether a column in a DataFrame can contain null values or not.\n",
    "\n",
    "# Itâ€™s defined in each StructField inside a StructType schema using the third argument:\n",
    "\n",
    "'''\n",
    "For example:\n",
    "StructType([StructField(\"column_name\", DataType(), nullable),])\n",
    "'''\n",
    "\n",
    "# If nullable is set to True, the column can contain null values; if set to False, it cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f2038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------+------+------------+\n",
      "|emp_id|           name|department|salary|joining_date|\n",
      "+------+---------------+----------+------+------------+\n",
      "|  1036|    Lisa Thomas|   Support| 82227|  2023-07-17|\n",
      "|  1099|  Holly Goodwin|   Finance| 97811|  2021-04-03|\n",
      "|  1019|     Aaron Neal|        IT| 54948|  2020-12-02|\n",
      "|  1003|    Jason Clark|   Support|108328|  2016-04-20|\n",
      "|  1090|  Beverly Clark| Marketing| 48881|  2019-03-21|\n",
      "|  1045|   Joel Roberts|   Support| 42300|  2023-09-08|\n",
      "|  1074| Richard Miller| Marketing| 94625|  2023-07-29|\n",
      "|  1056|      Ryan Hall|        IT| 54619|  2017-04-16|\n",
      "|  1037|     Linda Lane|Operations| 42007|  2018-04-28|\n",
      "|  1094|    April Young|   Support| 75014|  2021-12-10|\n",
      "|  1050|   Amber Savage|Operations|105312|  2023-06-04|\n",
      "|  1080|    Sandra King|   Finance| 42694|  2017-07-02|\n",
      "|  1033|   Ryan Kim DVM|Operations| 63050|  2021-11-07|\n",
      "|  1008|William Alvarez| Marketing|119709|  2020-03-05|\n",
      "|  1011|  Andrea Wright| Marketing| 63399|  2019-12-08|\n",
      "|  1053| Karen Robinson|   Support| 64075|  2016-08-27|\n",
      "|  1058|  Timothy Lucas|Operations| 95573|  2022-09-22|\n",
      "|  1070| Carrie Johnson|Operations| 82801|  2022-05-27|\n",
      "|  1038|   Michael Shaw|        IT|108888|  2021-10-20|\n",
      "|  1015|    Kevin Smith|   Finance| 97113|  2016-04-21|\n",
      "+------+---------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new dataframe with csv file\n",
    "\n",
    "df2 = spark.read.csv(r'/workspaces/pyspark/dataset/employee_data_v2.csv', header=True, inferSchema=True)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f455152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if inferSchema is set to True, Spark will automatically infer the data types of each column based on the data in the CSV file.\n",
    "# If set to False, all columns will be read as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6291ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou can also create a DataFrame like this, as shown below:\\ndata = [\\n    (1, \"Alice\", 30, \"Engineering\", 85000, \"2018-03-15\", \"Bangalore\", 4.5),\\n    (2, \"Bob\", 45, \"Sales\", 70000, \"2016-07-01\", \"Mumbai\", 3.8),\\n    (3, \"Charlie\", 28, \"Marketing\", 65000, \"2019-11-23\", \"Delhi\", 4.2),\\n    (4, \"David\", 35, \"Engineering\", 92000, \"2015-04-11\", \"Bangalore\", 4.7),\\n    (5, \"Eva\", 40, \"HR\", 60000, \"2017-02-01\", \"Pune\", 3.9),\\n    (6, \"Frank\", 29, \"Sales\", 68000, \"2020-09-20\", \"Chennai\", 4.1),\\n    (7, \"Grace\", 33, \"Engineering\", 87000, \"2018-12-12\", \"Hyderabad\", 4.6),\\n    (8, \"Hannah\", 31, \"Marketing\", 64000, \"2019-05-25\", \"Delhi\", 4.0),\\n    (9, \"Ian\", 38, \"Sales\", 72000, \"2014-10-10\", \"Mumbai\", 3.7),\\n    (10, \"Jenny\", 27, \"HR\", 59000, \"2021-01-01\", \"Bangalore\", 4.3)\\n]\\n\\ncolumns = [\"id\", \"name\", \"age\", \"department\", \"salary\", \"joining_date\", \"city\", \"performance_score\"]\\n\\ndf = spark.createDataFrame(data, columns)\\ndf.show()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "You can also create a DataFrame like this, as shown below:\n",
    "data = [\n",
    "    (1, \"Alice\", 30, \"Engineering\", 85000, \"2018-03-15\", \"Bangalore\", 4.5),\n",
    "    (2, \"Bob\", 45, \"Sales\", 70000, \"2016-07-01\", \"Mumbai\", 3.8),\n",
    "    (3, \"Charlie\", 28, \"Marketing\", 65000, \"2019-11-23\", \"Delhi\", 4.2),\n",
    "    (4, \"David\", 35, \"Engineering\", 92000, \"2015-04-11\", \"Bangalore\", 4.7),\n",
    "    (5, \"Eva\", 40, \"HR\", 60000, \"2017-02-01\", \"Pune\", 3.9),\n",
    "    (6, \"Frank\", 29, \"Sales\", 68000, \"2020-09-20\", \"Chennai\", 4.1),\n",
    "    (7, \"Grace\", 33, \"Engineering\", 87000, \"2018-12-12\", \"Hyderabad\", 4.6),\n",
    "    (8, \"Hannah\", 31, \"Marketing\", 64000, \"2019-05-25\", \"Delhi\", 4.0),\n",
    "    (9, \"Ian\", 38, \"Sales\", 72000, \"2014-10-10\", \"Mumbai\", 3.7),\n",
    "    (10, \"Jenny\", 27, \"HR\", 59000, \"2021-01-01\", \"Bangalore\", 4.3)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"department\", \"salary\", \"joining_date\", \"city\", \"performance_score\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
